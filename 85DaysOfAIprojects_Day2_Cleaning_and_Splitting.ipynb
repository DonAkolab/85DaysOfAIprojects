{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "85DaysOfAIprojects_Day2_Cleaning_and_Splitting.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DonAkolab/85DaysOfAIprojects/blob/master/85DaysOfAIprojects_Day2_Cleaning_and_Splitting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv4kP_GjhrqB",
        "colab_type": "text"
      },
      "source": [
        "# Cleaning and Splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a1aF9fP3fVR",
        "colab_type": "text"
      },
      "source": [
        "$ python -c \"import sklearn; print sklearn.__version__\"\n",
        "0.18.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1XJDRkNImvmY",
        "colab": {}
      },
      "source": [
        "## importing neccessary library\n",
        "\n",
        "import numpy as np                                            #provides support for more efficient numerical computation\n",
        "import pandas as pd                                           #a convenient library that supports dataframes\n",
        "\n",
        "from sklearn import preprocessing                             #preoprocessing module\n",
        "from sklearn.model_selection import train_test_split          #sampling helper\n",
        "\n",
        "\n",
        "\n",
        "#A \"family\" of models are broad types of models, such as random forests, SVM's, linear regression models\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor             #import the random forest family\n",
        "\n",
        "\n",
        "from sklearn.pipeline import make_pipeline                      # Import cross-validation pipelinePython\n",
        "from sklearn.model_selection import GridSearchCV  \n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score         #Import evaluation metricsPython\n",
        "from sklearn.externals import joblib                             #Import module for saving scikit-learn modelsPython\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA_S29ABy44V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_url = 'http://mlr.cs.umass.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
        "data = pd.read_csv(dataset_url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFH0Q7WYzNRE",
        "colab_type": "code",
        "outputId": "0be1f746-cc6b-4ba6-9594-f5905e0294d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "print (data.head())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  fixed acidity;\"volatile acidity\";\"citric acid\";\"residual sugar\";\"chlorides\";\"free sulfur dioxide\";\"total sulfur dioxide\";\"density\";\"pH\";\"sulphates\";\"alcohol\";\"quality\"\n",
            "0   7.4;0.7;0;1.9;0.076;11;34;0.9978;3.51;0.56;9.4;5                                                                                                                     \n",
            "1   7.8;0.88;0;2.6;0.098;25;67;0.9968;3.2;0.68;9.8;5                                                                                                                     \n",
            "2  7.8;0.76;0.04;2.3;0.092;15;54;0.997;3.26;0.65;...                                                                                                                     \n",
            "3  11.2;0.28;0.56;1.9;0.075;17;60;0.998;3.16;0.58...                                                                                                                     \n",
            "4   7.4;0.7;0;1.9;0.076;11;34;0.9978;3.51;0.56;9.4;5                                                                                                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jJGfcRrzPPz",
        "colab_type": "code",
        "outputId": "fc89881d-c2f5-40db-823c-e85d835e86b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Read CSV with semicolon separatorPython\n",
        "data = pd.read_csv(dataset_url, sep=';')\n",
        " \n",
        "print (data.head())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   fixed acidity  volatile acidity  citric acid  ...  sulphates  alcohol  quality\n",
            "0            7.4              0.70         0.00  ...       0.56      9.4        5\n",
            "1            7.8              0.88         0.00  ...       0.68      9.8        5\n",
            "2            7.8              0.76         0.04  ...       0.65      9.8        5\n",
            "3           11.2              0.28         0.56  ...       0.58      9.8        6\n",
            "4            7.4              0.70         0.00  ...       0.56      9.4        5\n",
            "\n",
            "[5 rows x 12 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDB4pv-KzdIL",
        "colab_type": "code",
        "outputId": "17749bde-fc3f-4e19-82b2-e899256c8389",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print (data.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1599, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwICn2LCzeEk",
        "colab_type": "code",
        "outputId": "05bf47f9-d7f0-4d63-e51c-706985aaea9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "print (data.describe())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       fixed acidity  volatile acidity  ...      alcohol      quality\n",
            "count    1599.000000       1599.000000  ...  1599.000000  1599.000000\n",
            "mean        8.319637          0.527821  ...    10.422983     5.636023\n",
            "std         1.741096          0.179060  ...     1.065668     0.807569\n",
            "min         4.600000          0.120000  ...     8.400000     3.000000\n",
            "25%         7.100000          0.390000  ...     9.500000     5.000000\n",
            "50%         7.900000          0.520000  ...    10.200000     6.000000\n",
            "75%         9.200000          0.640000  ...    11.100000     6.000000\n",
            "max        15.900000          1.580000  ...    14.900000     8.000000\n",
            "\n",
            "[8 rows x 12 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHI2L711z8la",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Separate target from training features Python\n",
        "y = data.quality\n",
        "X = data.drop('quality', axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3I0rrHI4DXD",
        "colab_type": "code",
        "outputId": "6583ad37-5b31-49ba-e92f-ee2de1170bf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "print (y.head())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    5\n",
            "1    5\n",
            "2    5\n",
            "3    6\n",
            "4    5\n",
            "Name: quality, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk7WwKoE4eKc",
        "colab_type": "code",
        "outputId": "13dc26af-0e1b-401d-b4e7-e42890692347",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "print (X.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   fixed acidity  volatile acidity  citric acid  ...    pH  sulphates  alcohol\n",
            "0            7.4              0.70         0.00  ...  3.51       0.56      9.4\n",
            "1            7.8              0.88         0.00  ...  3.20       0.68      9.8\n",
            "2            7.8              0.76         0.04  ...  3.26       0.65      9.8\n",
            "3           11.2              0.28         0.56  ...  3.16       0.58      9.8\n",
            "4            7.4              0.70         0.00  ...  3.51       0.56      9.4\n",
            "\n",
            "[5 rows x 11 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zcTdwW10Nlj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split data into train and test setsPython\n",
        "# random_state : However, if you use a particular value for random_state(random_state = 1 or any other value) everytime the result will be same,i.e, same values in train and test datasets\n",
        "# stratify : array-like or None (default=None) If not None, data is split in a stratified fashion, using this as the class labels.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123, stratify=y)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98902QyA6ach",
        "colab_type": "text"
      },
      "source": [
        "random_state : int, RandomState instance or None, optional (default=None)\n",
        "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdm746Ln0XB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standardization is the process of subtracting the means from each feature and then dividing by the feature standard deviations.\n",
        "# Standardization is a common requirement for machine learning tasks. \n",
        "# Many algorithms assume that all features are centered around zero and have approximately the same variance.\n",
        "\n",
        "# Lazy way of scaling dataPython\n",
        "X_train_scaled = preprocessing.scale(X_train)\n",
        "print (X_trained_scaled)\n",
        "\n",
        "print (X_train_scaled.mean(axis=0))\n",
        "\n",
        " \n",
        "print (X_train_scaled.std(axis=0))\n",
        "\n",
        "# but why did we say that we won't use this code?\n",
        "# The reason is that we won't be able to perform the exact same transformation on the test set.\n",
        "# Sure, we can still scale the test set separately, but we won't be using the same means and standard deviations as we used to transform the training set.\n",
        "# In other words, that means it wouldn't be a fair representation of how the model pipeline, include the preprocessing steps, would perform on brand new data.\n",
        "# Now, here's the preprocessing code we will use...\n",
        "# So instead of directly invoking the scale function, we'll be using a feature in Scikit-Learn called the Transformer API. The Transformer API allows you to \"fit\" a preprocessing step using the training data the same way you'd fit a model...\n",
        "# ...and then use the same transformation on future data sets!\n",
        "# Here's what that process looks like:\n",
        "# Fit the transformer on the training set (saving the means and standard deviations)\n",
        "# Apply the transformer to the training set (scaling the training data)\n",
        "# Apply the transformer to the test set (using the same means and standard deviations)\n",
        "# This makes your final estimate of model performance more realistic, and it allows to insert your preprocessing steps into a cross-validation pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm2aBn_l14gG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here's how you do it: Fitting the Transformer APIPython\n",
        "scaler = preprocessing.StandardScaler().fit(X_train)\n",
        "\n",
        "# Now, the scaler object has the saved means and standard deviations for each feature in the training set.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8cgm7qY9FYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let's confirm that worked:\n",
        "\n",
        "# Applying transformer to training dataPython\n",
        "X_train_scaled = scaler.transform(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLnwY-nj9OcE",
        "colab_type": "code",
        "outputId": "c6c5482c-731c-4bc6-f203-1f1a16835076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print (X_train_scaled.mean(axis=0))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1.16664562e-16 -3.05550043e-17 -8.47206937e-17 -2.22218213e-17\n",
            "  2.22218213e-17 -6.38877362e-17 -4.16659149e-18 -2.54439854e-15\n",
            " -8.70817622e-16 -4.08325966e-16 -1.17220107e-15]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IPNTd349Ol_",
        "colab_type": "code",
        "outputId": "15849d32-6d02-48e4-b70a-bdfb12e8a9dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print (X_train_scaled.std(axis=0))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5nsJJti1_fr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Note how we're taking the scaler object and using it to transform the training set. \n",
        "#Later, we can transform the test set using the exact same means and standard deviations used to transform the training set:\n",
        "#Applying transformer to test dataPython\n",
        "\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qDVjQJJKRuy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8efeff3d-3b1d-4a04-8567-f9e2290b5ad5"
      },
      "source": [
        "print (X_test_scaled.mean(axis=0))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.02776704  0.02592492 -0.03078587 -0.03137977 -0.00471876 -0.04413827\n",
            " -0.02414174 -0.00293273 -0.00467444 -0.10894663  0.01043391]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDpTEAhiKTdh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ae0ce182-b45c-4028-ca4e-68f60b91f85f"
      },
      "source": [
        "print (X_test_scaled.std(axis=0))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.02160495 1.00135689 0.97456598 0.91099054 0.86716698 0.94193125\n",
            " 1.03673213 1.03145119 0.95734849 0.83829505 1.0286218 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofenRu2-fRIs",
        "colab_type": "text"
      },
      "source": [
        "Notice how the scaled features in the test set are not perfectly centered at zero with unit variance! This is exactly what we'd expect, as we're transforming the test set using the means from the training set, not from the test set itself.\n",
        "In practice, when we set up the cross-validation pipeline, we won't even need to manually fit the Transformer API. \n",
        "Instead, we'll simply declare the class object, like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgB0j2Uk2Q_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Pipeline with preprocessing and modelPython\n",
        "pipeline = make_pipeline(preprocessing.StandardScaler(), RandomForestRegressor(n_estimators=100))\n",
        "\n",
        "#This is exactly what it looks like: a modeling pipeline that first transforms the data using StandardScaler() \n",
        "#and then fits a model using a random forest regressor."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWpwQcVofhdt",
        "colab_type": "text"
      },
      "source": [
        "There are two types of parameters we need to worry about: model parameters and hyperparameters. \n",
        "Models parameters can be learned directly from the data (i.e. regression coefficients), while hyperparameters cannot\n",
        "Hyperparameters express \"higher-level\" structural information about the model, and they are typically set before training the model.\n",
        "Within each decision tree, the computer can empirically decide where to create branches based on either mean-squared-error (MSE) or mean-absolute-error (MAE). Therefore, the actual branch locations are model parameters.\n",
        "\n",
        "However, the algorithm does not know which of the two criteria, MSE or MAE, that it should use. The algorithm also cannot decide how many trees to include in the forest. These are examples of hyperparameters that the user must set.\n",
        "\n",
        "We can list the tunable hyperparameters like so:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCxnsvuiBBEU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "b4b4180d-fc40-4050-be0f-7d4a84557d25"
      },
      "source": [
        "#List tunable hyperparametersPython\n",
        "print (pipeline.get_params())\n",
        "\n",
        "\n",
        "# ...\n",
        "# 'randomforestregressor__criterion': 'mse',\n",
        "# 'randomforestregressor__max_depth': None,\n",
        "# 'randomforestregressor__max_features': 'auto',\n",
        "# 'randomforestregressor__max_leaf_nodes': None,"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'memory': None, 'steps': [('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
            "                      max_features='auto', max_leaf_nodes=None,\n",
            "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                      min_samples_leaf=1, min_samples_split=2,\n",
            "                      min_weight_fraction_leaf=0.0, n_estimators=100,\n",
            "                      n_jobs=None, oob_score=False, random_state=None,\n",
            "                      verbose=0, warm_start=False))], 'verbose': False, 'standardscaler': StandardScaler(copy=True, with_mean=True, with_std=True), 'randomforestregressor': RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
            "                      max_features='auto', max_leaf_nodes=None,\n",
            "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                      min_samples_leaf=1, min_samples_split=2,\n",
            "                      min_weight_fraction_leaf=0.0, n_estimators=100,\n",
            "                      n_jobs=None, oob_score=False, random_state=None,\n",
            "                      verbose=0, warm_start=False), 'standardscaler__copy': True, 'standardscaler__with_mean': True, 'standardscaler__with_std': True, 'randomforestregressor__bootstrap': True, 'randomforestregressor__criterion': 'mse', 'randomforestregressor__max_depth': None, 'randomforestregressor__max_features': 'auto', 'randomforestregressor__max_leaf_nodes': None, 'randomforestregressor__min_impurity_decrease': 0.0, 'randomforestregressor__min_impurity_split': None, 'randomforestregressor__min_samples_leaf': 1, 'randomforestregressor__min_samples_split': 2, 'randomforestregressor__min_weight_fraction_leaf': 0.0, 'randomforestregressor__n_estimators': 100, 'randomforestregressor__n_jobs': None, 'randomforestregressor__oob_score': False, 'randomforestregressor__random_state': None, 'randomforestregressor__verbose': 0, 'randomforestregressor__warm_start': False}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6meHh2ZBrPM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#let's declare the hyperparameters we want to tune through cross-validation.\n",
        "\n",
        "#Declare hyperparameters to tunePython\n",
        "hyperparameters = { 'randomforestregressor__max_features' : ['auto', 'sqrt', 'log2'],\n",
        "                    'randomforestregressor__max_depth': [None, 5, 3, 1]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_avuOy5OfF3s",
        "colab_type": "text"
      },
      "source": [
        "Cross-validation is a process for reliably estimating the performance of a method for building a model by training and evaluating your model multiple times using the same method.These are the steps for CV:\n",
        "\n",
        "Split your data into k equal parts, or \"folds\" (typically k=10).\n",
        "Train your model on k-1 folds (e.g. the first 9 folds).\n",
        "Evaluate it on the remaining \"hold-out\" fold (e.g. the 10th fold).\n",
        "Perform steps (2) and (3) k times, each time holding out a different fold.\n",
        "Aggregate the performance across all k folds. This is your performance metric\n",
        "\n",
        "Here's how the CV pipeline looks after including preprocessing steps:\n",
        "\n",
        "Split your data into k equal parts, or \"folds\" (typically k=10).\n",
        "Preprocess k-1 training folds.\n",
        "Train your model on the same k-1 folds.\n",
        "Preprocess the hold-out fold using the same transformations from step (2).\n",
        "Evaluate your model on the same hold-out fold.\n",
        "Perform steps (2) - (5) k times, each time holding out a different fold.\n",
        "Aggregate the performance across all k folds. This is your performance metric.\n",
        "Fortunately, Scikit-Learn makes it stupidly simple to set this up:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ngu26Qt0B7xK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Sklearn cross-validation with pipelinePython\n",
        "clf = GridSearchCV(pipeline, hyperparameters, cv=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-N1kbv7f1d0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "3a2914e7-5d35-49c6-fae9-371396f655c6"
      },
      "source": [
        "# Fit and tune model\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('standardscaler',\n",
              "                                        StandardScaler(copy=True,\n",
              "                                                       with_mean=True,\n",
              "                                                       with_std=True)),\n",
              "                                       ('randomforestregressor',\n",
              "                                        RandomForestRegressor(bootstrap=True,\n",
              "                                                              criterion='mse',\n",
              "                                                              max_depth=None,\n",
              "                                                              max_features='auto',\n",
              "                                                              max_leaf_nodes=None,\n",
              "                                                              min_impurity_decrease=0.0,\n",
              "                                                              min_impurity_split=None,\n",
              "                                                              min_...\n",
              "                                                              min_weight_fraction_leaf=0.0,\n",
              "                                                              n_estimators=100,\n",
              "                                                              n_jobs=None,\n",
              "                                                              oob_score=False,\n",
              "                                                              random_state=None,\n",
              "                                                              verbose=0,\n",
              "                                                              warm_start=False))],\n",
              "                                verbose=False),\n",
              "             iid='warn', n_jobs=None,\n",
              "             param_grid={'randomforestregressor__max_depth': [None, 5, 3, 1],\n",
              "                         'randomforestregressor__max_features': ['auto', 'sqrt',\n",
              "                                                                 'log2']},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmgYgXjogNcM",
        "colab_type": "text"
      },
      "source": [
        "GridSearchCV essentially performs cross-validation across the entire \"grid\" (all possible permutations) of hyperparameters.\n",
        "It takes in your model (in this case, we're using a model pipeline), the hyperparameters you want to tune, and the number of folds to create\n",
        "you can see the best set of parameters found using CV:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bxZfxQIKSVM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "5c179441-2663-45f7-91cb-a0c3b958625c"
      },
      "source": [
        "print clf.best_params_\n",
        "# {'randomforestregressor__max_depth': None, 'randomforestregressor__max_features': 'auto'}"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-25-6a424f7ed5da>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print clf.best_params_\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(clf.best_params_)?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7aYOTT6gS7l",
        "colab_type": "text"
      },
      "source": [
        "After you've tuned your hyperparameters appropriately using cross-validation, you can generally get a small performance improvement by refitting the model on the entire training set.\n",
        "Conveniently, GridSearchCV from sklearn will automatically refit the model with the best set of hyperparameters using the entire training set.\n",
        "\n",
        "This functionality is ON by default, but you can confirm it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEUi05umKdUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Confirm model will be retrainedPython\n",
        "print clf.refit\n",
        "# True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63uZMmPGKmIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This step is really straightforward once you understand that the  clf object you used to tune the hyperparameters can also be used directly like a model object.\n",
        "\n",
        "#Here's how to predict a new set of data:\n",
        "#Predict a new set of dataPython\n",
        "y_pred = clf.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jSXqPfjKnbD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "175c4615-73db-4f3d-d0f0-327c01c205aa"
      },
      "source": [
        "print r2_score(y_test, y_pred)\n",
        "# 0.45044082571584243\n",
        " \n",
        "print mean_squared_error(y_test, y_pred)\n",
        "# 0.35461593750000003"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-7bbfe5473146>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print r2_score(y_test, y_pred)\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxsCqqH_Kqis",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "51b95827-a2f5-44e7-a13e-b6f5dd02c61f"
      },
      "source": [
        "#let's save your hard work so you can use the model in the future. It's really easy to do so:\n",
        "\n",
        "#Save model to a .pkl filePython\n",
        "joblib.dump(clf, 'rf_regressor.pkl')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rf_regressor.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqAEa2-sgzuN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "b7a3f492-a9ed-4a64-a8e8-ebd75764ecd8"
      },
      "source": [
        "\n",
        "#When you want to load the model again, simply use this function:\n",
        "\n",
        "Load model from .pkl filePython\n",
        "clf2 = joblib.load('rf_regressor.pkl')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-29-9cfe4d1487f6>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    Load model from .pkl filePython\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQyHVCQ_g0UV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict data set using loaded model\n",
        "clf2.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQbhcMz5K5nz",
        "colab_type": "text"
      },
      "source": [
        "## The complete code, from start to finish.\n",
        "Here's all the code in one place, in a single script.\n",
        "\n",
        "\n",
        "### 2. Import libraries and modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        " \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.externals import joblib \n",
        " \n",
        "### 3. Load red wine data.\n",
        "dataset_url = 'http://mlr.cs.umass.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
        "data = pd.read_csv(dataset_url, sep=';')\n",
        " \n",
        "### 4. Split data into training and test sets\n",
        "y = data.quality\n",
        "X = data.drop('quality', axis=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                    test_size=0.2, \n",
        "                                                    random_state=123, \n",
        "                                                    stratify=y)\n",
        " \n",
        "### 5. Declare data preprocessing steps\n",
        "pipeline = make_pipeline(preprocessing.StandardScaler(), \n",
        "                         RandomForestRegressor(n_estimators=100))\n",
        " \n",
        "### 6. Declare hyperparameters to tune\n",
        "hyperparameters = { 'randomforestregressor__max_features' : ['auto', 'sqrt', 'log2'],\n",
        "                  'randomforestregressor__max_depth': [None, 5, 3, 1]}\n",
        " \n",
        "### 7. Tune model using cross-validation pipeline\n",
        "clf = GridSearchCV(pipeline, hyperparameters, cv=10)\n",
        " \n",
        "clf.fit(X_train, y_train)\n",
        " \n",
        "### 8. Refit on the entire training set\n",
        " No additional code needed if clf.refit == True (default is True)\n",
        " \n",
        "### 9. Evaluate model pipeline on test data\n",
        "pred = clf.predict(X_test)\n",
        "print r2_score(y_test, pred)\n",
        "print mean_squared_error(y_test, pred)\n",
        " \n",
        "### 10. Save model for future use\n",
        "joblib.dump(clf, 'rf_regressor.pkl')\n",
        "\n",
        "### To load: \n",
        "clf2 = joblib.load('rf_regressor.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmFkkBnIkgyR",
        "colab_type": "text"
      },
      "source": [
        "#References\n",
        "\n",
        "https://elitedatascience.com/python-machine-learning-tutorial-scikit-learn\n"
      ]
    }
  ]
}